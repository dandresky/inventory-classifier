'''
This model is developed for development purposes only. I use it to test my
image processing and explore fully connected output configurations using a
smaller model architecture that can train relatively faster than the Deep
learning model I intend to use.

Model A uses a single output layer with a RELU activation function.
'''
import keras
from keras import losses
from keras import metrics
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from image_processing import ImageProcessing
from sklearn.model_selection import train_test_split


def get_model(filter_size=150, input_shape=(150,150,3)):
    model = Sequential()
    model.add(Conv2D(filter_size, (3, 3), padding='same',
                     input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(Conv2D(filter_size, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(2*filter_size, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(Conv2D(2*filter_size, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('relu'))

    # initiate RMSprop optimizer
    opt = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

    # Let's train the model using RMSprop
    model.compile(loss=losses.mean_squared_error,
                  optimizer=opt,
                  metrics=[metrics.mae])
    return model


def main():
    img_proc = ImageProcessing(batch_size=10,
                               target_size=(150,150))
    # create model
    model = get_model(150, (150,150,3))
    # This will do preprocessing and realtime data augmentation:
    train_datagen, val_datagen = img_proc.get_datagenerators_v1()

    # process batches of training images and train the model
    while img_proc.has_more_training_data():
        # get the next batch of processed images
        train_images, train_labels = img_proc.process_next_training_batch()
        #print(train_images.shape)
        #train_images /= 255

        # in my local test I am only pulling 10 images at a time. The model
        # batch size should therefore be 5
        batch_size = 5

        # Compute quantities required for feature-wise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        train_datagen.fit(train_images)

        # Fit the model on the batches generated by datagen.flow().
        # I am processing images in batches manually, rather than all at once,
        # therefore, epochs will default to 1
        model.fit_generator(train_datagen.flow(train_images, train_labels,
                                         batch_size=batch_size),
                            verbose=True,
                            epochs=1,
                            steps_per_epoch=1,
                            workers=4)

    while img_proc.has_more_test_data():
        # get the next batch of processed images
        test_images, test_labels = img_proc.process_next_test_batch()
        #test_images /= 255

        # in my local test I am only pulling 10 images at a time. The model
        # batch size should therefore be 5
        batch_size = 5

        # Score trained model.
        score = model.evaluate(test_images,
                               test_labels,
                               batch_size=batch_size,
                               verbose=True,
                               sample_weight=None)
        print('Test loss:', score[0])
        print('Test accuracy:', score[1])

    pass



if __name__ == '__main__':
    main()
